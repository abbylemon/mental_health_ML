{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cloud_etl.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WKuC-ZXnbSeU",
        "colab_type": "text"
      },
      "source": [
        "Make sure to go to the **Edit** menu and click **Clear all outputs** before and after running this notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G6DrGJY9ebHl",
        "colab_type": "text"
      },
      "source": [
        "# Load CSV files from S3 bucket into Spark dataframes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-_u8QZ7KWCPg",
        "colab_type": "text"
      },
      "source": [
        "## Start Spark Session"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ud3zn9HhUeC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Install Java, Spark, and Findspark\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q http://www-us.apache.org/dist/spark/spark-2.4.5/spark-2.4.5-bin-hadoop2.7.tgz\n",
        "!tar xf spark-2.4.5-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark\n",
        "\n",
        "# Set Environment Variables\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.5-bin-hadoop2.7\"\n",
        "\n",
        "# Start a SparkSession\n",
        "import findspark\n",
        "findspark.init()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8TPrNmGOdff6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget https://jdbc.postgresql.org/download/postgresql-42.2.9.jar"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xuGZMTqQdjhW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName(\"MentalHealthETL\").config(\"spark.driver.extraClassPath\",\"/content/postgresql-42.2.9.jar\").getOrCreate()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8VLwUxitWNc9",
        "colab_type": "text"
      },
      "source": [
        "## Mount Google Drive into this runtime\n",
        "\n",
        "To access the csv data files from the S3 bucket, you need to mount your google drive into this runtime. To do that, run the following cells.\n",
        "\n",
        "This will prompt a URL with an authentication code. After you go to the URL and insert that authentication code in the provided space, your google drive will be mounted."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "if7KU8FHWG5r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1rShythfYP-b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%cd /content/gdrive/My Drive/data_final_project/mental_health_ML"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IWEn1lbxe1yA",
        "colab_type": "text"
      },
      "source": [
        "## Create config.py file\n",
        "\n",
        "In the **mental_health_ML** directory, create a file called **config.py** and add the following contents:\n",
        "\n",
        "```bash\n",
        "ACCESS_ID='AWS_ACCESS_KEY_ID'\n",
        "ACCESS_KEY='AWS_SECRET_ACCESS_KEY'\n",
        "BUCKET_NAME='S3_BUCKET_NAME'\n",
        "```\n",
        "\n",
        "Replace AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY, and S3_BUCKET_NAME with their actual values. This file is in the .gitignore so that it won't be committed to GitHub."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VwKl8JK4fo9I",
        "colab_type": "text"
      },
      "source": [
        "## Access S3 bucket where csv files are stored"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fTqZtpDucToI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Install aws sdk for python\n",
        "! pip install boto3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wFcAyRQGR3jH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import boto3\n",
        "from config import ACCESS_ID, ACCESS_KEY, BUCKET_NAME\n",
        "\n",
        "# Use Amazon S3\n",
        "s3 = boto3.resource('s3', aws_access_key_id=ACCESS_ID, aws_secret_access_key= ACCESS_KEY)\n",
        "bucket_name = BUCKET_NAME\n",
        "\n",
        "# Bucket where csv files are stored.\n",
        "bucket = s3.Bucket(bucket_name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tViP-DZafv87",
        "colab_type": "text"
      },
      "source": [
        "## Read in data from S3 bucket and load into Spark dataframes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FcdlBhToiT_g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Read in data from S3 bucket\n",
        "from pyspark import SparkFiles\n",
        "\n",
        "bucket_name = BUCKET_NAME\n",
        "original_dataframes = {}\n",
        "\n",
        "for file in bucket.objects.all():\n",
        "  key = file.key\n",
        "  key_without_extension = key[:-4]\n",
        "  year = key_without_extension[-4:]\n",
        "  url=f\"https://{bucket_name}.s3.amazonaws.com/{key}\"\n",
        "  spark.sparkContext.addFile(url)\n",
        "  original_dataframes[year] = spark.read.csv(SparkFiles.get(key), sep=\",\", header=True, inferSchema=True)\n",
        "\n",
        "# Show DataFrame\n",
        "original_dataframes[\"2019\"].show(n=5)\n",
        "# original_dataframes[\"2018\"].show(n=5)\n",
        "# original_dataframes[\"2017\"].show(n=5)\n",
        "# original_dataframes[\"2016\"].show(n=5)\n",
        "# original_dataframes[\"2014\"].show(n=5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IEWskN7kfzqH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
